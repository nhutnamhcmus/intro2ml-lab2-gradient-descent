{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5-final"},"colab":{"name":"Lab02-GradientDescent.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"80cytuYSuyuz"},"source":["# Lab02: Gradient Descent.\n","\n","- Student ID: 18120061\n","- Student name: Le Nhut Nam"]},{"cell_type":"markdown","metadata":{"id":"UW5iBOIhuyu1"},"source":["**How to do your homework**\n","\n","\n","You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n","\n","You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n","\n","**How to submit your homework**\n","\n","Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n","\n","Then create a folder named `ID` (for example, if your ID is 1234567, then name the folder `1234567`) Copy file notebook to this folder, compress and submit it on moodle (**recommended compression format: zip**).\n","\n","**Contents:**\n","- Gradient descent.\n","\n","**Notes that**: this lab is prepared based on the book \"Deep Learning for Computer Vision with Python\" by Adrian Rosebrock."]},{"cell_type":"markdown","metadata":{"id":"Ghp7Cg7euyu2"},"source":["## 1. Loss landscape\n","![Loss lanscape](img.png) <center>**Figure 1. Loss landscape visualized as a 2D plot. Source: codecamp.vn**<center>\n","\n","&nbsp;&nbsp;&nbsp;&nbsp; The gradient descent method is an iterative optimization algorithm that operates over a loss landscape (also called an optimization surface).As we can see, our loss landscape has many peaks and valleys based on which values our parameters take on. Each peak is a local maximum that represents very high regions of loss – the local maximum with the largest loss across the entire loss landscape is the global maximum. Similarly, we also have local minimum which represents many small regions of loss. The local minimum with the smallest loss across the loss landscape is our global minimum. In an ideal world, we would like to find this global minimum, ensuring our parameters take on the most optimal possible values.\n"," \n","&nbsp;&nbsp;&nbsp;&nbsp;Each position along the surface of the corresponds to a particular loss value given a set of\n","parameters $\\mathbf{W}$ (weight matrix) and $\\mathbf{b}$ (bias vector). Our goal is to try different values of $\\mathbf{W}$ and $\\mathbf{b}$, evaluate their loss, and then take a step towards more optimal values that (ideally) have lower loss."]},{"cell_type":"markdown","metadata":{"id":"ugUR9D0Zuyu3"},"source":["## 2. The “Gradient” in Gradient Descent"]},{"cell_type":"markdown","metadata":{"id":"Aw-wiisluyu5"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;We can use $\\mathbf{W}$ and $\\mathbf{b}$ and to compute a loss function $L$ or we are able to find our relative position on the loss landscape, but **which direction** we should take a step to move closer to the minimum.\n","\n","- All We need to do is follow the slope of the gradient $\\nabla_{\\mathbf{W}}$. We can compute the gradient $\\nabla_{\\mathbf{W}}$ across all dimensions using the following equation:\n","$$\\dfrac{df\\left(x\\right)}{dx}=\\lim_{h\\to0} \\dfrac{f\\left(x+h\\right)-f\\left(x\\right)}{h}$$\n","- But, this equation has 2 problems:\n","    + 1. It’s an *approximation* to the gradient.\n","    + 2. It’s painfully slow.\n","    \n","&nbsp;&nbsp;&nbsp;&nbsp; In practice, we use the **analytic gradient** instead."]},{"cell_type":"markdown","metadata":{"id":"HbASKnSyuyu5"},"source":["## 3. Implementation"]},{"cell_type":"markdown","metadata":{"id":"YTn9OjQYuyu6"},"source":["### 3.1. Import library"]},{"cell_type":"code","metadata":{"id":"Q2Ri4LC2uyu7"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from sklearn.datasets import make_blobs\n","import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ya84WD_Auyu-"},"source":["### 3.2. Create data"]},{"cell_type":"code","metadata":{"id":"xxZPB5hyuyu_"},"source":["# generate a 2-class classification problem with 2,000 data points, each data point is a 2D feature vector\n","(X, y) = make_blobs(n_samples=2000, n_features=2, centers=2, cluster_std=1.5, random_state=1)\n","y = y.reshape((y.shape[0], 1))\n","\n","'''insert a column of 1’s as the last entry in the feature matrix  \n","--> treat the bias as a trainable parameter'''\n","X = np.c_[X, np.ones((X.shape[0]))]\n","\n","# Split data, use 50% of the data for training and the remaining 50% for testing\n","(trainX, testX, trainY, testY) = train_test_split(X, y, test_size=0.5, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bosr9l6fuyvC"},"source":["### 3.3. Training\n","#### Sigmoid function and derivative of the sigmoid function"]},{"cell_type":"code","metadata":{"id":"TDd0RJONuyvC"},"source":["def sigmoid_activation(x):\n","    \n","    \"\"\"compute the sigmoid activation value for a given input\"\"\"\n","    #TODO\n","    return \n","\n","def sigmoid_deriv(x):\n","    '''compute the derivative of the sigmoid function ASSUMING\n","    that the input ‘x‘ has already been passed through the sigmoid\n","    activation function'''\n","    #TODO\n","    return \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"09xCNUMmuyvF"},"source":["#### Compute output"]},{"cell_type":"code","metadata":{"id":"JOedv39ruyvG"},"source":["def compute_h(W, X):\n","    \"\"\"\n","    Compute output: Take the dot product between our features ‘X‘ and the weight\n","    matrix ‘W‘, then pass this value through our sigmoid activation function \n","    \"\"\"\n","    #TODO\n","    return \n","def predict(W, X):\n","    '''Take the dot product between our features and weight matrix, \n","       then pass this value through our sigmoid activation'''\n","    #TODO\n","    #preds=?\n","    \n","    # apply a step function to threshold the outputs to binary\n","    # class labels\n","    preds[preds <= 0.5] = 0\n","    preds[preds > 0] = 1\n","\n","    return preds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3kQCwnGYuyvJ"},"source":["#### Initialize our weight matrix and list of losses"]},{"cell_type":"code","metadata":{"id":"BIeit25BuyvJ"},"source":["W = np.random.randn(X.shape[1], 1)\n","losses = []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aF2u4fHZuyvO"},"source":["#### Compute gradient"]},{"cell_type":"code","metadata":{"id":"S4uoc9NquyvP"},"source":["def compute_gradient(error, h, trainX):\n","    #TODO \n","    \"\"\"\n","    The gradient descent update is the dot product between our\n","    features and the error of the sigmoid derivative of our predictions\n","    \"\"\"\n","   \n","    return gradient"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q4N1SN6euyvS"},"source":["#### Training function "]},{"cell_type":"code","metadata":{"id":"fQ0oHSZSuyvS"},"source":["def train(W,trainX, trainY, learning_rate, num_epochs):\n","    for epoch in np.arange(0, num_epochs):\n","        h=compute_h(W,trainX)\n","        # now that we have our predictions, we need to determine the\n","        # ‘error‘, which is the difference between our predictions and\n","        # the true values\n","        error = h - trainY\n","        loss = np.sum(error ** 2)\n","        losses.append(loss)\n","        gradient=compute_gradient(error, h, trainX)\n","        W += -learning_rate * gradient\n","        \n","        if epoch == 0 or (epoch + 1) % 5 == 0:\n","            print(\"Epoch={}, loss={:.7f}\".format(int(epoch + 1),loss))\n","        \n","    return W"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uN7uO4cxuyvV"},"source":["#### Train our model\n"]},{"cell_type":"code","metadata":{"id":"gw4C8vDPuyvV"},"source":["num_epochs=100\n","learning_rate=0.01\n","W=train(W,trainX, trainY, learning_rate, num_epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kTxdQU6GuyvZ"},"source":["#### Evaluate result"]},{"cell_type":"code","metadata":{"id":"zWGqYfjFuyva"},"source":["preds = predict(W, testX)\n","print(classification_report(testY, preds))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IiBkCQpGuyvf"},"source":["**TODO: Study about accuracy, recall, precision, f1-score.**\n","- Accuracy:\n","- Recall:\n","- Precision:\n","- F1:"]},{"cell_type":"code","metadata":{"id":"TiNckNH6uyvf"},"source":["plt.figure(figsize=(11.7,8.27))\n","plt.title(\"Data\")\n","plt.scatter(testX[:, 0], testX[:, 1], marker=\"o\", c=testY[:, 0], s=30)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jk2Xh1TGuyvh"},"source":["plt.figure(figsize=(11.7,8.27))\n","plt.plot(range(0, num_epochs), losses)\n","plt.title(\"Training Loss\")\n","plt.xlabel(\"Epoch #\")\n","plt.ylabel(\"Loss\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wa2EmWi6uyvj"},"source":["**TODO: Try out different learning rates. Give me your observations**"]},{"cell_type":"code","metadata":{"id":"DHO_hn71uyvk"},"source":[],"execution_count":null,"outputs":[]}]}